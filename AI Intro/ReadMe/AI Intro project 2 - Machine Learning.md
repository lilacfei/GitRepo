# AI Intro project 2 - Machine Learning

## 一、 线性回归 (Air quality dataset)

> 本次实验选择一组污染气体进行实验，不考虑每种气体之间的互相影响。本次实验的目的是配合温度，湿度等自然因素，根据传感器得到的数据去预测实际的数据。

### 1.缺失值处理

>AirQuality（以下简称AQ）数据集中将缺失的数据统一用-200表示，拉低了样本数据的均值。考虑用缺失值以外数据的均值替代缺失值。若传感器数据或者真实数据中存在-200便不计算该条数据。CO一组数据去除缺失值后还剩余7344条数据，NMHC一组数据去除缺失值后还剩余887条数据，NOx一组数据去除缺失值后还剩余7396条数据，NO2一组数据去除缺失值后还剩余7393条数据；因此，选择NOx一组数据进行线性回归的实验。

### 2.数据探索

> 利用箱型图对NOx的传感器测试数据和真实数据进行分析；

![](/pic/0.png)

![](/pic/1.png)



> NOx(GT)的1/4线在100附近，均值在200附近，3/4线在280附近；PT08.S3(NOx)的1/4线在600附近，均值在800附近，3/4线在1000附近；虽然数据大小不在一个维度上，但由图可知，数据的集中程度和分布情况基本一致。

### 3.训练模型

![](/pic/27.png)

![](/pic/28.png)

>  上面是对于测试数据的整体描述，下面是测试数据各个维度之间相关系数矩阵的可视化；

![](/pic/29.png)

> 先进行训练集和测试集的分割，用的是我自己实现的lib中的model_selection模块（仿效sklearn），返回的是训练集和测试集的数据以及训练集和测试集的标签。
>
> 再用lib中的多元线性回归模型对训练集进行训练，并在测试集上面测试。
>
> 最后用score方法评判线性回归的准确率。

## 二、 分类模型 (BLE&RSSI labeled dataset)

### 1.数据探索

> 首先要对数据集有个整体了解。

![](/pic/30.png)

![](/pic/3.png)

> 可见一共有13个基站，每个基站在不同时间都会接收到来自于不同位置的信号。本次实验要通过SVM，决策树以及随机森林的方式对这个数据集进行分类。
>
> 用直方图的形式更加清楚地了解信号基站的可能取值，以及取值的分布。

![](/pic/2.png)

> 可见，信号基站的取值大部分都为-200，即未探测到值；但也有小部分的值在-100到-50之间。

### 2.分类方法

> 要找到准确的分类方法不容易，需要一步步地去尝试。我尝试了三种分类方法，列举如下：
>
> - 直接进行训练
>
>   直接用所给的location进行训练的效果并不好，只有0.187的正确率；初步估计是因为location的分类过多，每一类的样本量过小，不具有可重复性。
>
>   e.x. 对location进行计数，并取前10项列举
>
>   ![](/pic/4.png)
>
>   

> - 选定区域分类
>
>   将平面图分为三个区域，入口区，左半区和右半区。
>
>   ![](/pic/5.jpg)
>
>   测试出来正确率是1.0，惊讶！但实际上是因为分区过于粗糙，将大部分数据都化为一类，所以测试数据都为一个类别。

> - 计算到入口的曼哈顿距离
>
>   取location中的x轴和y轴出来，x轴是A-U，y轴是1-18，入口的坐标是D-9，计算曼哈顿距离。一共算出来23个不同的值，前10个计数最多的列举如下：
>
>   ![](/pic/7.png)
>
>   基本符合分类条件，开始训练模型。

### 3.训练模型

> 先对数据进行归一化，用sklearn中的preprocessing模块，将数据调整为符合标准正态分布。
>
> - SVM模型
>
>   ![](/pic/6.png)
>
>   ![](/pic/8.png)
>
>   ![](/pic/9.png)
>
>   其实本次试验中SVM的表现并不是很好，也许跟这次试验的数据不线性可分有关；
>
>   Pic1：对于距离出口曼哈顿距离为16和18的点可以达到相对高的正确率和召回率，而19的准确率并不高，但它的召回率很高，说明其本身标注为真的数据不多，本次模型都将其正确的标注找到了；
>
>   Pic2：此张图将正确的分类和期望的分类放在一起，画成了一张折线图。可以说，原始数据上下波动不大，但预测数据波动很大，可能这与原始数据少有关；
>
>   Pic3：这是混淆矩阵的热力图，可谓是万黑丛中一抹红。说明SVM的效果的确不好。
>
> - 决策树模型
>
>   ![](/pic/10.png)
>
>   ![](/pic/11.png)
>
>   ![](/pic/13.png)
>
>   本次试验中决策树模型比SVM模型更好，因为它具有随机性，不受到线性约束。
>
>   Pic1：对于距离出口曼哈顿距离为12、16、18、20与22的点可以达到相对高的正确率和召回率，24、25的准确率为1，但召回率较低，说明只要预测到的值将其正确性判断正确，却没有找到剩下来应该是24、25的值；
>
>   Pic2：与SVM相差不大；
>
>   Pic3：混淆矩阵的热力图比SVM更好，在中心线上有颜色。
>
> - 随机森林模型
>
>   ![](/pic/14.png)
>
>   ![](/pic/15.png)
>
>   ![](/pic/16.png)
>
>   随机森林相当于集成的决策树，会继承其优点，并由更加显著的正确率；
>
>   Pic1：对于距离出口曼哈顿距离为12、16、18、20与22的点还有24、25两点的特性与决策树完全一致，随机森林最高正确率达到0.69，表现更加均衡；
>
>   Pic2：与SVM相差不大；
>
>   Pic3：混淆矩阵的热力图颜色更鲜艳。

# 三、 聚类分析 (BLE&RSSI unlabeled dataset)

### 1.数据降维

> 用tSNE的方法，将数据转化为二维数据；
>
> ![](/pic/17.png)
>
> 这个二维数据综合了所有13个基站的信息，代表着两个大基站接收到来自一个坐标的信号。
>
> 可视化如下图：
>
> ![](/pic/18.png)
>
> 分别用不同的聚类方法对其进行训练，挑选出针对这个数据集最优的一种聚类方法。
>
> - DBSCAN
>
>   ![](/pic/19.png)
>
>   ![](/pic/20.png)
>
>   DBSCAN算法能发现任意形状的聚簇，聚类结果几乎不依赖于节点遍历顺序，能够有效发现噪声点。但是本次实验中eps显然比较小，使得密度小的cluster被划分成多个性质相似的cluster，导致图中的点并没有被完全划分开来，甚至可以说是仍旧混在一起，聚类效果并不好；
>
> - KMeans
>
>   ![](/pic/21.png)
>
>   ![](/pic/22.png)
>
>   KMeans对每一个数据点，会计算该点与每组中心点的距离，将该点归类到距离中心最近的组。各组所有向量的均值是每一组的新中心点。重复步骤，直到不再发生变化。可见，KMeans在本次实验中表现优秀；
>
> - GMM
>
>   ![](/pic/23.png)
>
>   ![](/pic/24.png)
>
>   高斯混合模型GMM理论上可以拟合出任意类型的分布，通常解决同一集合下的数据包含多个不同的分布的情况。但是本次试验中已经进行归一化，归为正态分布，所以出现只有一种分类的情况；
>
> - AgglomerativeClustering
>
>   ![](/pic/25.png)
>
>   ![](/pic/26.png)
>
>   层次聚类法试图在不同层次对数据集进行划分，形成树形的聚类结构。将数据集中每个样本看作一个初始聚类簇，每一次找出距离最近的两个聚类簇进行合并，直到达到顶设的聚类簇个数。默认为2，所以最后剩下两种颜色的分类。

