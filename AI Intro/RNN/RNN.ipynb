{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import jieba\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "train_file = 'train_data.txt'\n",
    "test_file = 'test_data.txt'\n",
    "\n",
    "vocab_file = 'vocab.txt'\n",
    "category_file = 'category.txt'\n",
    "output_folder = './run_text_rnn'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_embedding_size = 20,\n",
    "        num_timesteps = 50,\n",
    "        num_lstm_nodes = [32, 32],\n",
    "        num_lstm_layers = 2,\n",
    "        num_fc_nodes = 32,\n",
    "        batch_size = 100,\n",
    "        clip_lstm_grads = 1.0,\n",
    "        learning_rate = 0.001,\n",
    "        num_word_threshold = 5\n",
    "    )\n",
    "hps = get_default_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab_size: 27672\n",
      "INFO:tensorflow:num_classes: 8\n",
      "INFO:tensorflow:label: news, id: 1\n"
     ]
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, filename, num_word_threshold):\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "        self._read_dict(filename)\n",
    "    \n",
    "    def _read_dict(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, frequency = line.strip('\\r\\n').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._num_word_threshold:\n",
    "                continue\n",
    "            idx = len(self._word_to_id)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            self._word_to_id[word] = idx\n",
    "    \n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self._unk)\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "    \n",
    "    def sentence_to_id(self, sentence):\n",
    "        word_ids = [self.word_to_id(cur_word) \\\n",
    "                    for cur_word in jieba.cut(sentence)]\n",
    "        return word_ids\n",
    "\n",
    "\n",
    "class CategoryDict:\n",
    "    def __init__(self, filename):\n",
    "        self._category_to_id = {}\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category = line.strip('\\r\\n')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._category_to_id)\n",
    "        \n",
    "    def category_to_id(self, category):\n",
    "        if not category in self._category_to_id:\n",
    "            raise Exception(\"%s is not in our category list\" % category)\n",
    "        return self._category_to_id[category]\n",
    "        \n",
    "vocab = Vocab(vocab_file, hps.num_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "tf.logging.info('vocab_size: %d' % vocab_size)\n",
    "\n",
    "category_vocab = CategoryDict(category_file)\n",
    "num_classes = category_vocab.size()\n",
    "tf.logging.info('num_classes: %d' % num_classes)\n",
    "test_str = 'news'\n",
    "tf.logging.info(\n",
    "    'label: %s, id: %d' % (\n",
    "        test_str,\n",
    "        category_vocab.category_to_id(test_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading data from train_data.txt\n",
      "INFO:tensorflow:Loading data from test_data.txt\n",
      "4067\n",
      "1854\n",
      "(array([[   10, 10403,     0, 24362,     0,     0,  5723,  6554,  7529,\n",
      "         4739,   983,    13, 10403,     0,  4739,     0,    75,    73,\n",
      "           25,    14,    36,    20,   289,  1034,    32, 19125,   462,\n",
      "            1,     0, 15097,   218,  2187,  3699,     3, 10403,     0,\n",
      "            0,   600,     0,     0,  2187,   284,   152,    37,  3031,\n",
      "           56,    20,     1,  1115,     2],\n",
      "       [   10,   182,    21,   234, 16221,  1361,   114,     0,   113,\n",
      "         1782,    34,     6,  1853,     7,   528,    22,    14,    55,\n",
      "          164,    27,    83,   182,  7341,   122,   182,  2401,  1339,\n",
      "           49,    20,   398,    21,    30,   182,   246,  5773,   947,\n",
      "        16221,     1,  1361,   877,    35,  1318,     1, 12209,   690,\n",
      "           72,   234,    66,  1782,  1822]]), array([2, 1]))\n",
      "(array([[   10,  1703,     0, 23845,  3842,   564,   170,    90,  1048,\n",
      "           66, 15740,   345,  9719,    13,     0,  5087,   808,  1901,\n",
      "            0,    49,    69,    74,   279,     1,  5556,  5926,    21,\n",
      "          326,   330,     1,     0,     4,     0,     4,     0,     0,\n",
      "           65,     0,  6812,     1,   601,   521,  6290,  6091,   340,\n",
      "        13796,     1,   819,  6787,  2368],\n",
      "       [   10,  2801,    13,     0,   246,  1429,     0,  2603,     0,\n",
      "        19572,  7565,  6153,   218,    27,     0,    75,    73,    22,\n",
      "           14,   106,    20,   326,     1,    53,   207,   127,     2,\n",
      "            0,  1972,    78,    61,     1,  1022,   611,  6153,     5,\n",
      "          218,     2,     3,    61,     1,  1319,     0,    56,     0,\n",
      "         1092,   595,     0,  3861,   830]]), array([1, 2]))\n"
     ]
    }
   ],
   "source": [
    "class TextDataSet:\n",
    "    def __init__(self, filename, vocab, category_vocab, num_timesteps):\n",
    "        self._vocab = vocab\n",
    "        self._category_vocab = category_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        # matrix\n",
    "        self._inputs = []\n",
    "        # vector\n",
    "        self._outputs = []\n",
    "        self._indicator = 0\n",
    "        self._parse_file(filename)\n",
    "    \n",
    "    def _parse_file(self, filename):\n",
    "        tf.logging.info('Loading data from %s', filename)\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            label = line.strip('\\r\\n').split(',')[0]\n",
    "            content = \"\"\n",
    "            for one in line.strip('\\r\\n').split(',')[1:]:\n",
    "                content += one\n",
    "            #print(content)\n",
    "            id_label = self._category_vocab.category_to_id(label)\n",
    "            #print(id_label)\n",
    "            id_words = self._vocab.sentence_to_id(content)\n",
    "            #print(id_words)\n",
    "            id_words = id_words[0: self._num_timesteps]\n",
    "            padding_num = self._num_timesteps - len(id_words)\n",
    "            id_words = id_words + [\n",
    "                self._vocab.unk for i in range(padding_num)]\n",
    "            self._inputs.append(id_words)\n",
    "            self._outputs.append(id_label)\n",
    "        self._inputs = np.asarray(self._inputs, dtype = np.int32)\n",
    "        self._outputs = np.asarray(self._outputs, dtype = np.int32)\n",
    "        self._random_shuffle()\n",
    "        self._num_examples = len(self._inputs)\n",
    "    \n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._outputs = self._outputs[p]\n",
    "    \n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            raise Exception(\"batch_size: %d is too large\" % batch_size)\n",
    "        \n",
    "        batch_inputs = self._inputs[self._indicator: end_indicator]\n",
    "        batch_outputs = self._outputs[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_inputs, batch_outputs\n",
    "            \n",
    "train_dataset = TextDataSet(\n",
    "    train_file, vocab, category_vocab, hps.num_timesteps) \n",
    "test_dataset = TextDataSet(\n",
    "    test_file, vocab, category_vocab, hps.num_timesteps)\n",
    "\n",
    "print(train_dataset.num_examples())\n",
    "print(test_dataset.num_examples())\n",
    "\n",
    "print(train_dataset.next_batch(2))\n",
    "print(test_dataset.next_batch(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading data from train_data.txt\n",
      "INFO:tensorflow:Loading data from test_data.txt\n",
      "(array([[   10, 12109,  4510, ...,  8534,    13,   643],\n",
      "       [   10,  2068,    74, ...,   105,  5193,   890],\n",
      "       [   10,     0,    79, ...,     2,     0,   158],\n",
      "       ...,\n",
      "       [   10,   191,   301, ...,   382,    53, 18920],\n",
      "       [   10,    80,    26, ..., 17173,     4,   593],\n",
      "       [   10, 23145,  2591, ..., 15021,  9832,  1664]]), array([1, 1, 3, 1, 1, 1, 0, 2, 5, 1, 3, 3, 3, 1, 1, 2, 6, 1, 5, 1, 1, 1,\n",
      "       5, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 3, 3, 2, 1, 1, 0, 1, 1, 1,\n",
      "       3, 1, 1, 2, 3, 1, 1, 3, 1, 2, 3, 2, 5, 2, 3, 1, 1, 0, 1, 1, 1, 1,\n",
      "       3, 3, 1, 5, 1, 2, 1, 5, 1, 1, 1, 2, 0, 3, 3, 1, 1, 2, 1, 1, 0, 1,\n",
      "       3, 1, 1, 0, 5, 3, 1, 1, 1, 1, 1, 5]))\n",
      "(array([[   10,  1927,   107, ...,     1,   756, 14886],\n",
      "       [   10,   914,    11, ...,  2302,  1827,   277],\n",
      "       [   10,     0,     0, ...,     3,   115,    25],\n",
      "       ...,\n",
      "       [   10, 11665,   215, ...,   450,   200,  3486],\n",
      "       [   10,     0,   170, ...,    26,  4068,  6163],\n",
      "       [   10,   478,     0, ...,  2555,     0,   517]]), array([2, 2, 1, 3, 1, 3, 1, 1, 1, 1, 3, 3, 0, 1, 3, 1, 1, 3, 1, 5, 2, 1,\n",
      "       3, 1, 0, 1, 1, 2, 3, 1, 3, 1, 1, 3, 1, 3, 3, 2, 1, 1, 2, 3, 1, 1,\n",
      "       2, 5, 1, 1, 3, 1, 2, 2, 2, 2, 1, 3, 1, 2, 1, 3, 3, 1, 6, 1, 2, 2,\n",
      "       1, 5, 1, 1, 2, 3, 1, 3, 5, 2, 1, 3, 1, 1, 1, 1, 0, 3, 2, 6, 2, 1,\n",
      "       1, 3, 1, 0, 1, 0, 6, 1, 1, 3, 3, 3]))\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataSet(\n",
    "    train_file, vocab, category_vocab, hps.num_timesteps)\n",
    "test_dataset = TextDataSet(\n",
    "    test_file, vocab, category_vocab, hps.num_timesteps)\n",
    "print(train_dataset.next_batch(100))\n",
    "print(test_dataset.next_batch(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hps, vocab_size, num_classes):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "    \n",
    "    inputs = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    outputs = tf.placeholder(tf.int32, (batch_size, ))\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob') #dropout, 有百分之n的可能保留值\n",
    "    \n",
    "    global_step = tf.Variable(\n",
    "        tf.zeros([], tf.int64), name='global_step', trainable=False)\n",
    "    \n",
    "    with tf.name_scope('embedding'):\n",
    "        embeddings = tf.Variable(tf.random_uniform([vocab_size, hps.num_embedding_size], -1, 1))\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "    \n",
    "    scale = 1.0/math.sqrt(hps.num_embedding_size + hps.num_lstm_nodes[-1])/3.0\n",
    "    \n",
    "    lstm_init = tf.random_uniform_initializer(-scale,scale)\n",
    "    with tf.variable_scope('lstm_nn', initializer=lstm_init, reuse=tf.AUTO_REUSE):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell( #LSTM node\n",
    "                hps.num_lstm_nodes[i],\n",
    "                state_is_tuple = True)\n",
    "            cell = tf.contrib.rnn.DropoutWrapper( #dropout layer\n",
    "                cell,\n",
    "                output_keep_prob = keep_prob)\n",
    "            cells.append(cell) #add up\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells) #封装\n",
    "        \n",
    "        initial_state = cell.zero_state(hps.batch_size, tf.float32)\n",
    "        #rnn_outputs: [batch_size, num_timesteps, lstm_outputs]\n",
    "        rnn_outputs, _ = tf.nn.dynamic_rnn(\n",
    "            cell, embed_inputs,initial_state=initial_state) #包含了每一步的输出\n",
    "        last = rnn_outputs[:, -1, :] #每一个batch_size取最后一个输出\n",
    "    \n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('fc', initializer=fc_init, reuse=tf.AUTO_REUSE):\n",
    "        fc = tf.layers.dense(last, hps.num_fc_nodes, name='fc1')\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc, keep_prob) #简单起见，用相同的keep_prob\n",
    "        logits = tf.layers.dense(fc1_dropout, num_classes, name='fc2') #将输出映射到类别上\n",
    "    \n",
    "    with tf.name_scope('metrics'):\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits( #对label做one_hot, 对logits做softmax，之后将两者进行交叉熵损失函数计算\n",
    "            logits = logits, labels = outputs\n",
    "        )\n",
    "        loss = tf.reduce_mean(softmax_loss)\n",
    "        # [0, 1, 5, 4, 2] -> softmax:2\n",
    "        y_pred = tf.argmax(\n",
    "            tf.nn.softmax(logits),1,output_type=tf.int32)\n",
    "        correct_pred = tf.equal(outputs, y_pred)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) \n",
    "    \n",
    "    with tf.name_scope('train_op'): #优化\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            tf.logging.info('variable name: %s' % (var.name))\n",
    "        #截断梯度\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(loss,tvars), hps.clip_lstm_grads)\n",
    "        #clips_lstm_grad 限制梯度\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars),global_step=global_step)\n",
    "        return ((inputs, outputs, keep_prob),\n",
    "                (loss, accuracy),\n",
    "                (train_op, global_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\11780\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-10-8bd4917fc225>:24: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-10-8bd4917fc225>:29: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-10-8bd4917fc225>:34: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\11780\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-10-8bd4917fc225>:37: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From <ipython-input-10-8bd4917fc225>:39: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "INFO:tensorflow:variable name: embedding/Variable:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc1/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc1/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc2/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc2/bias:0\n",
      "WARNING:tensorflow:From C:\\Users\\11780\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "placeholders, metrics, others = create_model(\n",
    "    hps, vocab_size, num_classes)\n",
    "inputs, outputs, keep_prob = placeholders\n",
    "loss, accuracy = metrics\n",
    "train_op, global_step = others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_holdout(sess, accuracy, dataset_for_test, batch_size):\n",
    "    num_batches = dataset_for_test.num_examples() // batch_size\n",
    "    tf.logging.info(\"Eval holdout: num_examples = %d, batch_size = %d\",\n",
    "                    dataset_for_test.num_examples(), batch_size)\n",
    "    accuracy_vals = []\n",
    "    for i in range(num_batches):\n",
    "        batch_inputs, batch_labels = dataset_for_test.next_batch(batch_size)\n",
    "        accuracy_val = sess.run(accuracy,\n",
    "                                feed_dict = {\n",
    "                                    inputs: batch_inputs,\n",
    "                                    outputs: batch_labels,\n",
    "                                    keep_prob: 1.0,\n",
    "                                })\n",
    "        accuracy_vals.append(accuracy_val)\n",
    "    return np.mean(accuracy_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step:    50, loss: 1.309, accuracy: 0.590\n",
      "INFO:tensorflow:Step:   100, loss: 1.375, accuracy: 0.490\n",
      "INFO:tensorflow:Step:   150, loss: 1.279, accuracy: 0.570\n",
      "INFO:tensorflow:Step:   200, loss: 0.905, accuracy: 0.770\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:   200, test_accuracy: 0.551\n",
      "INFO:tensorflow:Step:   250, loss: 0.788, accuracy: 0.740\n",
      "INFO:tensorflow:Step:   300, loss: 0.705, accuracy: 0.720\n",
      "INFO:tensorflow:Step:   350, loss: 0.570, accuracy: 0.800\n",
      "INFO:tensorflow:Step:   400, loss: 0.426, accuracy: 0.890\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:   400, test_accuracy: 0.625\n",
      "INFO:tensorflow:Step:   450, loss: 0.349, accuracy: 0.880\n",
      "INFO:tensorflow:Step:   500, loss: 0.513, accuracy: 0.840\n",
      "INFO:tensorflow:Step:   550, loss: 0.220, accuracy: 0.950\n",
      "INFO:tensorflow:Step:   600, loss: 0.292, accuracy: 0.910\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:   600, test_accuracy: 0.653\n",
      "INFO:tensorflow:Step:   650, loss: 0.511, accuracy: 0.820\n",
      "INFO:tensorflow:Step:   700, loss: 0.265, accuracy: 0.930\n",
      "INFO:tensorflow:Step:   750, loss: 0.227, accuracy: 0.940\n",
      "INFO:tensorflow:Step:   800, loss: 0.246, accuracy: 0.910\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:   800, test_accuracy: 0.651\n",
      "INFO:tensorflow:Step:   850, loss: 0.166, accuracy: 0.940\n",
      "INFO:tensorflow:Step:   900, loss: 0.139, accuracy: 0.960\n",
      "INFO:tensorflow:Step:   950, loss: 0.239, accuracy: 0.940\n",
      "INFO:tensorflow:Step:  1000, loss: 0.104, accuracy: 0.980\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  1000, test_accuracy: 0.642\n",
      "INFO:tensorflow:Step:  1050, loss: 0.238, accuracy: 0.930\n",
      "INFO:tensorflow:Step:  1100, loss: 0.152, accuracy: 0.950\n",
      "INFO:tensorflow:Step:  1150, loss: 0.318, accuracy: 0.910\n",
      "INFO:tensorflow:Step:  1200, loss: 0.164, accuracy: 0.960\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  1200, test_accuracy: 0.641\n",
      "INFO:tensorflow:Step:  1250, loss: 0.110, accuracy: 0.980\n",
      "INFO:tensorflow:Step:  1300, loss: 0.148, accuracy: 0.970\n",
      "INFO:tensorflow:Step:  1350, loss: 0.157, accuracy: 0.960\n",
      "INFO:tensorflow:Step:  1400, loss: 0.118, accuracy: 0.970\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  1400, test_accuracy: 0.641\n",
      "INFO:tensorflow:Step:  1450, loss: 0.152, accuracy: 0.970\n",
      "INFO:tensorflow:Step:  1500, loss: 0.135, accuracy: 0.960\n",
      "INFO:tensorflow:Step:  1550, loss: 0.167, accuracy: 0.950\n",
      "INFO:tensorflow:Step:  1600, loss: 0.070, accuracy: 0.980\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  1600, test_accuracy: 0.643\n",
      "INFO:tensorflow:Step:  1650, loss: 0.059, accuracy: 0.980\n",
      "INFO:tensorflow:Step:  1700, loss: 0.069, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  1750, loss: 0.244, accuracy: 0.920\n",
      "INFO:tensorflow:Step:  1800, loss: 0.061, accuracy: 0.970\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  1800, test_accuracy: 0.646\n",
      "INFO:tensorflow:Step:  1850, loss: 0.050, accuracy: 0.980\n",
      "INFO:tensorflow:Step:  1900, loss: 0.116, accuracy: 0.970\n",
      "INFO:tensorflow:Step:  1950, loss: 0.080, accuracy: 0.960\n",
      "INFO:tensorflow:Step:  2000, loss: 0.049, accuracy: 0.980\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  2000, test_accuracy: 0.647\n",
      "INFO:tensorflow:Step:  2050, loss: 0.143, accuracy: 0.950\n",
      "INFO:tensorflow:Step:  2100, loss: 0.094, accuracy: 0.970\n",
      "INFO:tensorflow:Step:  2150, loss: 0.046, accuracy: 0.980\n",
      "INFO:tensorflow:Step:  2200, loss: 0.025, accuracy: 1.000\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  2200, test_accuracy: 0.633\n",
      "INFO:tensorflow:Step:  2250, loss: 0.083, accuracy: 0.970\n",
      "INFO:tensorflow:Step:  2300, loss: 0.067, accuracy: 0.970\n",
      "INFO:tensorflow:Step:  2350, loss: 0.036, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  2400, loss: 0.036, accuracy: 0.990\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  2400, test_accuracy: 0.623\n",
      "INFO:tensorflow:Step:  2450, loss: 0.048, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  2500, loss: 0.017, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  2550, loss: 0.009, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  2600, loss: 0.007, accuracy: 1.000\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  2600, test_accuracy: 0.629\n",
      "INFO:tensorflow:Step:  2650, loss: 0.042, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  2700, loss: 0.011, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  2750, loss: 0.031, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  2800, loss: 0.075, accuracy: 0.970\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  2800, test_accuracy: 0.613\n",
      "INFO:tensorflow:Step:  2850, loss: 0.057, accuracy: 0.980\n",
      "INFO:tensorflow:Step:  2900, loss: 0.019, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  2950, loss: 0.013, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  3000, loss: 0.078, accuracy: 0.960\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  3000, test_accuracy: 0.641\n",
      "INFO:tensorflow:Step:  3050, loss: 0.048, accuracy: 0.980\n",
      "INFO:tensorflow:Step:  3100, loss: 0.009, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  3150, loss: 0.069, accuracy: 0.960\n",
      "INFO:tensorflow:Step:  3200, loss: 0.013, accuracy: 1.000\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  3200, test_accuracy: 0.627\n",
      "INFO:tensorflow:Step:  3250, loss: 0.035, accuracy: 0.980\n",
      "INFO:tensorflow:Step:  3300, loss: 0.063, accuracy: 0.950\n",
      "INFO:tensorflow:Step:  3350, loss: 0.058, accuracy: 0.970\n",
      "INFO:tensorflow:Step:  3400, loss: 0.016, accuracy: 0.990\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  3400, test_accuracy: 0.633\n",
      "INFO:tensorflow:Step:  3450, loss: 0.054, accuracy: 0.980\n",
      "INFO:tensorflow:Step:  3500, loss: 0.050, accuracy: 0.980\n",
      "INFO:tensorflow:Step:  3550, loss: 0.109, accuracy: 0.960\n",
      "INFO:tensorflow:Step:  3600, loss: 0.030, accuracy: 0.980\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  3600, test_accuracy: 0.639\n",
      "INFO:tensorflow:Step:  3650, loss: 0.021, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  3700, loss: 0.124, accuracy: 0.970\n",
      "INFO:tensorflow:Step:  3750, loss: 0.041, accuracy: 0.980\n",
      "INFO:tensorflow:Step:  3800, loss: 0.031, accuracy: 0.980\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  3800, test_accuracy: 0.638\n",
      "INFO:tensorflow:Step:  3850, loss: 0.024, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  3900, loss: 0.022, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  3950, loss: 0.006, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  4000, loss: 0.001, accuracy: 1.000\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  4000, test_accuracy: 0.643\n",
      "INFO:tensorflow:Step:  4050, loss: 0.065, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  4100, loss: 0.018, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  4150, loss: 0.045, accuracy: 0.980\n",
      "INFO:tensorflow:Step:  4200, loss: 0.026, accuracy: 0.990\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  4200, test_accuracy: 0.651\n",
      "INFO:tensorflow:Step:  4250, loss: 0.034, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  4300, loss: 0.004, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  4350, loss: 0.035, accuracy: 0.980\n",
      "INFO:tensorflow:Step:  4400, loss: 0.032, accuracy: 0.980\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  4400, test_accuracy: 0.631\n",
      "INFO:tensorflow:Step:  4450, loss: 0.032, accuracy: 0.970\n",
      "INFO:tensorflow:Step:  4500, loss: 0.017, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  4550, loss: 0.031, accuracy: 0.980\n",
      "INFO:tensorflow:Step:  4600, loss: 0.078, accuracy: 0.960\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  4600, test_accuracy: 0.628\n",
      "INFO:tensorflow:Step:  4650, loss: 0.027, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  4700, loss: 0.045, accuracy: 0.970\n",
      "INFO:tensorflow:Step:  4750, loss: 0.035, accuracy: 0.980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step:  4800, loss: 0.032, accuracy: 0.980\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  4800, test_accuracy: 0.647\n",
      "INFO:tensorflow:Step:  4850, loss: 0.026, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  4900, loss: 0.030, accuracy: 0.980\n",
      "INFO:tensorflow:Step:  4950, loss: 0.002, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  5000, loss: 0.017, accuracy: 0.990\n",
      "INFO:tensorflow:Eval holdout: num_examples = 1854, batch_size = 100\n",
      "INFO:tensorflow:Step:  5000, test_accuracy: 0.631\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "glostep = []\n",
    "loss_v = []\n",
    "acc_v = []\n",
    "t_acc_v = []\n",
    "train_keep_prob_value = 0.8\n",
    "test_keep_prob_value = 1.0\n",
    "\n",
    "num_train_steps = 5000\n",
    "#test\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(\"d:\\Documents\\Jupyter notebook\\AI Project3\",sess.graph)\n",
    "    sess.run(init_op)\n",
    "    for i in range(num_train_steps):\n",
    "        batch_inputs, batch_labels = train_dataset.next_batch(\n",
    "            hps.batch_size)\n",
    "        outputs_val = sess.run([loss, accuracy, train_op, global_step],\n",
    "                           feed_dict = {\n",
    "                               inputs: batch_inputs,\n",
    "                               outputs: batch_labels,\n",
    "                               keep_prob: train_keep_prob_value\n",
    "                           })\n",
    "        loss_val, accuracy_val, _, global_step_val = outputs_val\n",
    "        if global_step_val % 50 == 0:\n",
    "            tf.logging.info(\"Step: %5d, loss: %3.3f, accuracy: %3.3f\"\n",
    "                            % (global_step_val, loss_val, accuracy_val))\n",
    "            glostep.append(global_step_val)\n",
    "            loss_v.append(loss_val)\n",
    "            acc_v.append(accuracy_val)\n",
    "        if global_step_val % 200 == 0:\n",
    "            accuracy_test = eval_holdout(sess, accuracy, test_dataset, hps.batch_size)\n",
    "            tf.logging.info(\"Step: %5d, test_accuracy: %3.3f\"\n",
    "                            % (global_step_val, accuracy_test))\n",
    "            t_acc_v.append(accuracy_test)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig=plt.figure()  #生成图片框架\n",
    "ax=fig.add_subplot(1,1,1)  #连续性的画图\n",
    "ax.scatter(glostep,acc_v)\n",
    "parameter = np.polyfit(glostep, acc_v, 2)\n",
    "f = np.poly1d(parameter)\n",
    "ax.plot(glostep,f(glostep),c='r')\n",
    "plt.ion() #不会show一下就停止显示（python新功能）\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
