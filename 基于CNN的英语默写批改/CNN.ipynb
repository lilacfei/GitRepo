{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from load_img import read_img_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './my_cnn_model'\n",
    "TRAINING_STEPS = 10000\n",
    "FILELIST = [str(x) for x in ('d','b','c','a','e','f','g','h','p','j','k','l','m','n','o','i','q','r','s','t','u','v','w','x','y','z')]\n",
    "#FILELIST = ['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '3', '4', '5', '6', '7', '8', '9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    #tf.reset_default_graph()\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1,1,28,28])\n",
    "    input_layer = tf.transpose(input_layer, perm=[0,2,3,1])\n",
    "    # conv1_1： 神经元图， feature_map, 输出图像\n",
    "    conv1_1 = tf.layers.conv2d(input_layer, \n",
    "                             28, # output channel number\n",
    "                             (3,3), # kernel size\n",
    "                             padding='valid',\n",
    "                             activation=tf.nn.relu,\n",
    "                             name='conv1_1')\n",
    "    # conv1_2： 神经元图， feature_map, 输出图像\n",
    "    conv1_2 = tf.layers.conv2d(conv1_1, \n",
    "                             28, # output channel number\n",
    "                             (3,3), # kernel size\n",
    "                             padding='valid',\n",
    "                             activation=tf.nn.relu,\n",
    "                             name='conv1_2')\n",
    "\n",
    "    # 14 * 14\n",
    "    pooling1 = tf.layers.max_pooling2d(conv1_2,\n",
    "                                       (2,2), # kernel size\n",
    "                                       (2,2), # stride\n",
    "                                    name='pool1')\n",
    "\n",
    "    # conv2_1： 神经元图、 feature_map, 输出图像\n",
    "    conv2_1 = tf.layers.conv2d(pooling1, \n",
    "                             28, # output channel number\n",
    "                             (3,3), # kernel size\n",
    "                             padding='valid',\n",
    "                             activation=tf.nn.relu,\n",
    "                             name='conv2_1')\n",
    "    # conv2_2： 神经元图、 feature_map, 输出图像\n",
    "    conv2_2 = tf.layers.conv2d(conv2_1, \n",
    "                             28, # output channel number\n",
    "                             (3,3), # kernel size\n",
    "                             padding='valid',\n",
    "                             activation=tf.nn.relu,\n",
    "                             name='conv2_2')\n",
    "\n",
    "    # 7 * 7\n",
    "    pooling2 = tf.layers.max_pooling2d(conv2_2,\n",
    "                                       (2,2), # kernel size\n",
    "                                       (2,2), # stride\n",
    "                                    name='pool2')\n",
    "\n",
    "    flatten = tf.layers.flatten(pooling2)\n",
    "    dense = tf.layers.dense(inputs=flatten, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout( \n",
    "        inputs=dense, rate=0.2, training=mode == tf.estimator.ModeKeys.TRAIN) \n",
    "    # 全连接层,输出\n",
    "    logits = tf.layers.dense(inputs=dropout, units=26)\n",
    "    \n",
    "    predictions = { \n",
    "       # Generate predictions (for PREDICT and EVAL mode) \n",
    "       \"classes\": tf.argmax(input=logits, axis=1), \n",
    "       # Add `softmax_tensor` to the graph. It is used for PREDICT and by the \n",
    "       # `logging_hook`. \n",
    "       \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\") \n",
    "    } \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT: \n",
    "     # sess = tf.InteractiveSession() \n",
    "     # print('logits:',logits.eval(session = sess)) \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions) \n",
    " \n",
    "    # cross entropy\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    # y_ -> softmax\n",
    "    # y -> one_hot\n",
    "    # loss = ylogy_\n",
    "    \n",
    "     # Configure the Training Op (for TRAIN mode) \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN: \n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001) \n",
    "        train_op = optimizer.minimize( \n",
    "            loss=loss, \n",
    "            global_step=tf.train.get_global_step()) \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)  \n",
    "    \n",
    "    eval_metric_ops = { \n",
    "        \"accuracy\": tf.metrics.accuracy( \n",
    "            labels=labels, predictions=predictions[\"classes\"])} \n",
    "    return tf.estimator.EstimatorSpec( \n",
    "            mode=mode, loss=loss, eval_metric_ops=eval_metric_ops) \n",
    "\n",
    "   # Add evaluation metrics (for EVAL mode) \n",
    "\n",
    "    with tf.name_scope('train_op'):\n",
    "        train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "        #梯度下降的变种"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_symbol_classifier = tf.estimator.Estimator( \n",
    "    model_fn=cnn_model_fn, model_dir=MODEL_DIR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn_model(steps):\n",
    "    train_data,train_data_labels = read_img_file('train')\n",
    "\n",
    "    # set up logging for predictions\n",
    "    # log the values in the \"softmax\" tensor with label \"probabilities\"\n",
    "    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "    logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "    # train the model\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": train_data},\n",
    "        y=train_data_labels,\n",
    "        batch_size=100,\n",
    "        num_epochs=None,\n",
    "        shuffle=True)\n",
    "    #print(train_input_fn)\n",
    "    cnn_symbol_classifier.train(\n",
    "        input_fn=train_input_fn,\n",
    "        steps=steps,\n",
    "        hooks=[logging_hook])\n",
    "\n",
    "def eval_cnn_model():\n",
    "    eval_data, eval_data_labels = read_img_file('eval')\n",
    "    # evaluate the model and print results\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": eval_data},\n",
    "        y=eval_data_labels,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "    eval_results = cnn_symbol_classifier.evaluate(input_fn=eval_input_fn)\n",
    "    print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,train_data_labels = read_img_file('train')\n",
    "eval_data,eval_data_labels = read_img_file('test')\n",
    "\n",
    "# set up logging for predictions\n",
    "# log the values in the \"softmax\" tensor with label \"probabilities\"\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "#train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "     x={\"x\": train_data},\n",
    "     y=train_data_labels,\n",
    "     batch_size=100,\n",
    "     num_epochs=None,\n",
    "     shuffle=True)\n",
    "# print(train_input_fn)\n",
    "cnn_symbol_classifier.train(\n",
    "     input_fn=train_input_fn,\n",
    "     steps=TRAINING_STEPS,\n",
    "     hooks=[logging_hook])\n",
    "\n",
    "# evaluate the model and print results\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": eval_data},\n",
    "    y=eval_data_labels,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "eval_results = cnn_symbol_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from img2bin import read_img_and_convert_to_binary\n",
    "from img2bin import binary_img_segment\n",
    "from img2bin import normalize_matrix_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '1.jpg'\n",
    "img = cv2.imread(filename)\n",
    "original_img, binary_img = read_img_and_convert_to_binary(img)\n",
    "symbols = binary_img_segment(binary_img, original_img)\n",
    "symbols_to_be_predicted = normalize_matrix_value([x['src_img'] for x in symbols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn( \n",
    "     x={\"x\": np.array(symbols_to_be_predicted)}, \n",
    "     shuffle=False) \n",
    "predictions = cnn_symbol_classifier.predict(input_fn=predict_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(estimated_probabilities): \n",
    "    #print(estimated_probabilities)\n",
    "    indexes_of_top_n_largest_probability = np.argsort(-estimated_probabilities)\n",
    "    #print(indexes_of_top_n_largest_probability)\n",
    "    candidates = [] \n",
    "    for i in range(26):\n",
    "        index = indexes_of_top_n_largest_probability[i] \n",
    "        print(index)\n",
    "        candidates.append({'symbol':FILELIST[index],'probability':estimated_probabilities[index]}) \n",
    "    return candidates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = [] \n",
    "for i,p in enumerate(predictions): \n",
    "    candidates = get_candidates(p['probabilities'])\n",
    "    characters.append({'candidates':candidates}) \n",
    "    print([x['candidates'] for x in characters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for one in characters:\n",
    "    print(one['candidates'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
